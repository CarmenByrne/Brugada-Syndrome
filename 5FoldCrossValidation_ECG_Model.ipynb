{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f67de3ee",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe57ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import gc\n",
    "import keras\n",
    "import mcfly\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import auc, roc_curve, precision_recall_curve, brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "from datetime import datetime \n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "import math\n",
    "import scipy.stats as st\n",
    "from scipy import stats\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6081361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set to True if want to train the models during cv, False to load pretrained cv models\n",
    "train_models = False\n",
    "\n",
    "#Put same weights as in original model\n",
    "set_weights = True \n",
    "\n",
    "#Number of crossvalidation folds\n",
    "n_folds = 5\n",
    "\n",
    "#path to file with indexes of files split into training, val and test\n",
    "split_path = \"400_dumped/Final_Data/split/train_val_test.json\"\n",
    "\n",
    "#paths to the labels and the data\n",
    "labels_path = \"400_dumped/Final_Data/labels/labels.npy\"\n",
    "samples_path = \"400_dumped/Final_Data/samples/\"\n",
    "\n",
    "#paths to store and retrieve model types, architectures and hyperparameters\n",
    "archi_path = \"400_dumped/Models_Final_Data/architecture/architecture_\"\n",
    "params_path = \"400_dumped/Models_Final_Data/parameters/params_\"\n",
    "type_path = \"400_dumped/Models_Final_Data/type/type_\"\n",
    "\n",
    "#choose model to laod and train\n",
    "model_name = \"Model6.json\"\n",
    "\n",
    "#path to trained cv models\n",
    "cv_trained_path = \"400_dumped/CrossValidation/Model6/Round\" \n",
    "cv_t_test_path = \"400_dumped/CrossValidation/Model6/t_test_data.csv\"\n",
    "\n",
    "#path to original json data, to check filter types\n",
    "path_negative = \"AnonymisedECGs_json/negative\"\n",
    "path_positive = \"AnonymisedECGs_json/positive\"\n",
    "\n",
    "#set the seed \n",
    "random.seed(0) #generation of train, val, test sets\n",
    "np.random.seed(0) #mcfly models\n",
    "tf.random.set_seed(0) #keras training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e69164d",
   "metadata": {},
   "source": [
    "# Dictionary with sample id and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce798887",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_array = np.load(labels_path)\n",
    "labels = dict()\n",
    "\n",
    "for row in labels_array:\n",
    "    labels[row[0]] = int(row[1])\n",
    "\n",
    "    \n",
    "del labels_array\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb20ae0",
   "metadata": {},
   "source": [
    "# Train, val, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66a6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open original train, val, test split to calculate original weights\n",
    "with open(split_path, \"r\") as fp:\n",
    "    train_val_test_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814f4588",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_val_test_dict[\"train\"]) + len(train_val_test_dict[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a46702",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_val_test_dict[\"val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5da721e",
   "metadata": {},
   "source": [
    "# Cross Validation Set Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a534d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put together train and validation sets and shuffle\n",
    "train = train_val_test_dict[\"train\"].copy()\n",
    "val = train_val_test_dict[\"val\"].copy()\n",
    "\n",
    "cv_samples = train\n",
    "cv_samples.extend(val)\n",
    "\n",
    "#randomly shuffle train and val\n",
    "random.Random(10).shuffle(cv_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc571666",
   "metadata": {},
   "source": [
    "# Check filter distribution in cv samples dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b1b696",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_filters = pd.DataFrame()\n",
    "neg_filters = pd.DataFrame()\n",
    "\n",
    "for elem in cv_samples: \n",
    "    \n",
    "    if elem[0] == str(1):\n",
    "            directory = path_negative + \"/\"+ elem + \".json\"\n",
    "            \n",
    "    if elem[0] == str(2):\n",
    "        directory = path_positive + \"/\" + elem + \".json\"\n",
    "    \n",
    "    f = open(directory)\n",
    "    data = json.load(f)\n",
    "        \n",
    "    ecg = data[\"RestingECG\"]\n",
    "    waveform = pd.DataFrame(ecg[\"Waveform\"])\n",
    "    waveform_rhythm = pd.DataFrame(waveform[waveform[\"WaveformType\"]==\"Rhythm\"])\n",
    "    \n",
    "    label = \"\"\n",
    "    if \"positive\" in directory:\n",
    "        label = \"positive\"\n",
    "    elif \"negative\" in directory:\n",
    "        label = \"negative\"\n",
    "    \n",
    "    temp = pd.DataFrame(\n",
    "    {\n",
    "        \"high_pass\": waveform_rhythm[\"HighPassFilter\"],\n",
    "        \"low_pass\": waveform_rhythm[\"LowPassFilter\"],\n",
    "        \"ac\": waveform_rhythm[\"ACFilter\"],\n",
    "        \"label\": label\n",
    "    })\n",
    "    \n",
    "    if label == \"positive\":\n",
    "        pos_filters = pd.concat([pos_filters, temp])\n",
    "    elif label == \"negative\":\n",
    "        neg_filters = pd.concat([neg_filters, temp])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ebc4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_filter_dist(df): \n",
    "    filter_combo = df.groupby([\"high_pass\", \"low_pass\", \"ac\", \"label\"]).size().reset_index(name=\"Count\")\n",
    "    filter_combo[\"percentage_by_class\"] = 100 * filter_combo[\"Count\"] / filter_combo.groupby(\"label\")[\"Count\"].transform(\"sum\")\n",
    "    filter_combo[\"combination\"] = list(zip(filter_combo.high_pass, filter_combo.low_pass, filter_combo.ac))\n",
    "    filter_combo = filter_combo.sort_values(by=[\"label\", \"percentage_by_class\"], ascending=False)\n",
    "    \n",
    "    return filter_combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1129e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_filter_combo = analyse_filter_dist(pos_filters)\n",
    "n_filter_combo = analyse_filter_dist(neg_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1feb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_filter_combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ccf0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filter_combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ace5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_filter_combo = p_filter_combo.set_index(\"combination\")\n",
    "p_filter_combo = p_filter_combo.reindex(index = n_filter_combo[\"combination\"])\n",
    "p_filter_combo = p_filter_combo.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb220e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_filter_combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62401540",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filter_combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca350494",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filter_combo_head = n_filter_combo.head(5)\n",
    "p_filter_combo_head = p_filter_combo.head(5)\n",
    "\n",
    "p_filter_combo_head = p_filter_combo_head.set_index(\"combination\")\n",
    "p_filter_combo_head = p_filter_combo_head.reindex(index = n_filter_combo_head[\"combination\"])\n",
    "p_filter_combo_head = p_filter_combo_head.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4009e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.arange(p_filter_combo_head.shape[0])\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 12.5))\n",
    "rects_neg = ax.bar(ind - width/2, n_filter_combo_head[\"percentage_by_class\"], width, label = \"Negative\")\n",
    "rects_pos = ax.bar(ind + width/2, p_filter_combo_head[\"percentage_by_class\"], width, label = \"Positive\")\n",
    "ax.set_ylabel(\"Percentage of samples\")\n",
    "ax.set_title(\"Top 5 percentage of samples per filter combination per class\")\n",
    "ax.set_xticks(ind)\n",
    "y_labels = list(n_filter_combo_head[\"combination\"])\n",
    "ax.set_xticklabels(y_labels)\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e6819",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff1 = pd.merge(n_filter_combo[[\"combination\", \"percentage_by_class\"]],\n",
    "                p_filter_combo[[\"combination\", \"percentage_by_class\"]],\n",
    "                how = \"outer\",\n",
    "                left_on = [\"combination\"],\n",
    "                right_on = [\"combination\"],\n",
    "                suffixes = [\"_neg\", \"_pos\"])\n",
    "\n",
    "diff1.fillna(0, inplace=True)\n",
    "diff1[\"difference\"] = diff1[\"percentage_by_class_neg\"]- diff1[\"percentage_by_class_pos\"]\n",
    "\n",
    "diff1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7171e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution is similar to training set, hence no correcting for filters necessary\n",
    "# percentage of 16, 150, 50 increased since not corrected in val, but size of val is too small for significant effect on train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2d647",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6feacf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make n sets for n-fold crossval\n",
    "def cv_splits(cv_samples, n_folds):\n",
    "    cv_sets = {}\n",
    "    n_elements = round(len(cv_samples)/n_folds)\n",
    "    for idx in range(n_folds):\n",
    "        first_idx = idx *n_elements\n",
    "        last_idx = min(first_idx + n_elements, len(cv_samples))      \n",
    "        cv_sets[idx] = cv_samples[first_idx : last_idx]\n",
    "    return cv_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f6cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create validation and test sets and store in memory\n",
    "def set_generation(val_or_test, cv_train_val_test_dict, labels, dim = (2500, 8)):\n",
    "    n_samples = len(cv_train_val_test_dict[val_or_test])\n",
    "\n",
    "    #Initialise\n",
    "    X = np.empty((n_samples, dim[0], dim[1]))\n",
    "    y = np.empty((n_samples), dtype = int)\n",
    "\n",
    "    #Generate data\n",
    "    for i, ID in enumerate(cv_train_val_test_dict[val_or_test]):\n",
    "        #store sample\n",
    "        X[i,] = np.load(samples_path + ID +\".npy\")\n",
    "\n",
    "        #store class\n",
    "        y[i] = labels[ID]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c25c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate batches of data\n",
    "class DataGenerator(keras.utils.Sequence):    \n",
    "\n",
    "    def __init__(self, list_IDs, labels, batch_size = 32, dim = (2500,), n_channels = 8, n_classes=2, shuffle = True):\n",
    "        #\"Initialization\"\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        #number of batches per epoch\n",
    "        return int(np.floor(len(self.list_IDs)/self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Generates indexes of one batch of data\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        #find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        \n",
    "        #Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)        \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        #updates indexes after each epoch\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        #Generates data containing batch_size samples\n",
    "        \n",
    "        #Initialise\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype = int)\n",
    "        \n",
    "        #Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            #store sample\n",
    "            X[i,] = np.load(samples_path + ID +\".npy\")\n",
    "            \n",
    "            #store class\n",
    "            y[i] = self.labels[ID]\n",
    "        \n",
    "        return X, keras.utils.to_categorical(y, num_classes = self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474cc0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate class imbalance in original training set to keep same model\n",
    "def calculate_original_weights(train_val_test_dict):\n",
    "    zeroes = 0\n",
    "    ones = 0\n",
    "    for i, ID in enumerate(train_val_test_dict[\"train\"]):\n",
    "        if labels[ID] == 0:\n",
    "            zeroes = zeroes + 1\n",
    "        if labels[ID] == 1:\n",
    "            ones = ones + 1\n",
    "\n",
    "    if ones < zeroes:\n",
    "        class_weights = {0: 1., 1: zeroes/ones}\n",
    "    elif zeroes < ones:\n",
    "        class_weights = {0: ones/zeroes, 1: 1.}\n",
    "    else:\n",
    "        class_weights = {0: 1., 1: 1.}\n",
    "\n",
    "    return class_weights  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0234184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(cv_round, cv_train_val_test_dict, labels, class_weights, X_val, y_val, cv_trained_path):\n",
    "    #define parameters\n",
    "    params = {\"dim\" : (2500,),\n",
    "             \"batch_size\": 32,\n",
    "             \"n_classes\": 2,\n",
    "             \"n_channels\":8,\n",
    "             \"shuffle\" :True}\n",
    "\n",
    "    #Generators \n",
    "    training_generator = DataGenerator(cv_train_val_test_dict[\"train\"], labels, **params) \n",
    "    \n",
    "    #load model architecture\n",
    "    with open(archi_path + model_name, \"r\") as f:\n",
    "        model_loaded = json.load(f)\n",
    "        model = keras.models.model_from_json(model_loaded)    \n",
    "       \n",
    "    #set metrics\n",
    "    metric = [\"accuracy\"]\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer = \"adam\", metrics = metric)     \n",
    "    \n",
    "    #print time    \n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(current_time)\n",
    "    \n",
    "    #train\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 5, restore_best_weights = True)\n",
    "    history = model.fit(training_generator, \n",
    "              validation_data = (X_val, y_val), \n",
    "              epochs = 20,\n",
    "              class_weight = class_weights, \n",
    "              callbacks = callback,\n",
    "              verbose = True)\n",
    "    \n",
    "    #print time\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(current_time)\n",
    "    \n",
    "    #save the model\n",
    "    cv_path = cv_trained_path + model_name\n",
    "    cv_path = cv_trained_path + str(cv_round) + \".json\"\n",
    "    model.save(cv_path)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076086ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metrics(y_true, y_pred, y_proba, metrics):\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion matrix: \")\n",
    "    print(conf_mat)\n",
    "    tn,fp,fn,tp = conf_mat.ravel()\n",
    "    print(\"tn: \", tn,\" fp: \", fp,\" fn: \", fn,\" tp: \", tp)\n",
    "    \n",
    "    print(\"\")\n",
    "    matthews = ((tp*tn) - (fp*fn)) / math.sqrt(((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "    print(\"Matthews Correlation Coefficient: \", matthews)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    print(\"\")           \n",
    "    precision_bis = tp/(tp+fp) #positive predictive value\n",
    "    recall_bis = tp/(tp+fn)\n",
    "    f1 = 2*precision_bis*recall_bis/(precision_bis+recall_bis)\n",
    "    specificity = tn/(tn+fp) #true negative rate\n",
    "    fnr = fn/(fn+tp)\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "    FPR = fp/(fp+tn)\n",
    "    \n",
    "    print(\"precision/positive predictive value: \", precision_bis)\n",
    "    print(\"recall/sensitivity: \", recall_bis)\n",
    "    print(\"specificity/true negative rate: \", specificity)\n",
    "    print(\"False negative rate: \", fnr)\n",
    "    print(\"False positive rate: \", FPR)\n",
    "    print(\"accuracy: \", accuracy)    \n",
    "    print(\"f1 score: \", f1) \n",
    "\n",
    "      \n",
    "    print(\"\")\n",
    "    brier = brier_score_loss(y_true, y_proba)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
    "    auc_coef = auc(fpr, tpr)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    auprc = auc(recall, precision)\n",
    "    print(\"brier score: \", brier )\n",
    "    print(\"auc: \", auc_coef)\n",
    "    print(\"auprc: \", auprc)\n",
    "    \n",
    "    metrics.append([tn, fp, fn, tp, matthews, precision_bis, recall_bis, specificity, fnr, FPR, accuracy, f1, auc_coef, auprc, brier])\n",
    "    \n",
    "    return \n",
    "\n",
    "\n",
    "#predicts on cv test set and gets performance stats\n",
    "def predictions(model, X_test, y_test, metrics):\n",
    "    pred_probas = model.predict(X_test)\n",
    "\n",
    "    #BrS appears as 1, hence transformed to [0,1] => the second column returns 1 if BrS, 0 otherwise\n",
    "    BrS = y_test[:,1]\n",
    "    \n",
    "    #get probabilities and predictions\n",
    "    BrS_probas = pred_probas[:,1]\n",
    "    BrS_predictions = pred_probas.argmax(axis = -1)\n",
    "    BrS_predictions\n",
    "    \n",
    "    #get performance metrics\n",
    "    performance_metrics(BrS, BrS_predictions, BrS_probas, metrics)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e4c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterates over crossval sets, makes train, val and test sets\n",
    "#trains model\n",
    "def cross_val(cv_samples, n_folds, labels, set_weights, train_val_test_dict, cv_trained_path):\n",
    "    \n",
    "    #make n different sets from data\n",
    "    cv_sets = cv_splits(cv_samples, n_folds)\n",
    "    #make list of indexes of each cv fold\n",
    "    all_idxs = np.arange(n_folds)\n",
    "    \n",
    "    #calculate weights\n",
    "    if set_weights:  \n",
    "        class_weights = calculate_original_weights(train_val_test_dict)\n",
    "    else:\n",
    "        class_weights = {0: 1., 1: 1.}  \n",
    "        \n",
    "    #declare list to store performance metrics\n",
    "    metrics = []\n",
    "    \n",
    "    #cross validation, n rounds, train or load model & calculate performance metrics\n",
    "    for cv_round in range(n_folds):\n",
    "        cv_train_val_test_dict = {}\n",
    "        cv_train_val_test_dict[\"train\"] = []\n",
    "        \n",
    "        #start val set as 0th set from cv sets and test as val set +1, step size 1 per cv round\n",
    "        #val_idx = cv_round % n_folds\n",
    "        #test_idx = (cv_round+1) % n_folds        \n",
    "        #cv_train_val_test_dict[\"val\"] = cv_sets[val_idx]\n",
    "        #cv_train_val_test_dict[\"test\"] = cv_sets[test_idx]\n",
    "        \n",
    "        val_test_idx = cv_round % n_folds\n",
    "        val_test_set = cv_sets[val_test_idx]\n",
    "        val = random.sample(val_test_set, round(len(val_test_set) * 0.5)) #half of samples in this fold go to val\n",
    "        test = list(set(val_test_set).symmetric_difference(val)) #remove val samples from fold, to get 50% of data in test\n",
    "        cv_train_val_test_dict[\"val\"] = val\n",
    "        cv_train_val_test_dict[\"test\"] = test\n",
    "        \n",
    "        #all set indexes different to val and test become train\n",
    "        #train_idxs = all_idxs[(all_idxs != val_idx) & (all_idxs != test_idx)]\n",
    "        train_idxs = all_idxs[all_idxs != val_test_idx]  \n",
    "        \n",
    "        for train_key in train_idxs:\n",
    "            cv_train_val_test_dict[\"train\"].extend(cv_sets[train_key])\n",
    "        \n",
    "        if train_models:\n",
    "            #generate val set and save to memory\n",
    "            X_val, y_val = set_generation(\"val\", cv_train_val_test_dict, labels, (2500, 8))\n",
    "            y_val = keras.utils.to_categorical(y_val, 2)            \n",
    "            \n",
    "            #train model\n",
    "            model = model_training(cv_round, cv_train_val_test_dict, labels, class_weights, X_val, y_val, cv_trained_path)\n",
    "            \n",
    "            #free memory\n",
    "            del X_val, y_val\n",
    "            gc.collect()\n",
    "            \n",
    "        else:\n",
    "            #load trained model \n",
    "            cv_path = cv_trained_path + str(cv_round) + \".json\"\n",
    "            model = keras.models.load_model(cv_path)\n",
    "            \n",
    "        #generate test set and save to memory\n",
    "        X_test, y_test = set_generation(\"test\", cv_train_val_test_dict, labels, (2500, 8))\n",
    "        y_test = keras.utils.to_categorical(y_test, 2)\n",
    "        \n",
    "        #predict and get performance metrics\n",
    "        predictions(model, X_test, y_test, metrics)\n",
    "        \n",
    "        #free memory\n",
    "        del X_test, y_test\n",
    "        gc.collect()\n",
    "            \n",
    "    return metrics  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d3a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = cross_val(cv_samples, n_folds, labels, set_weights, train_val_test_dict, cv_trained_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a0b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates 95% CI\n",
    "def conf_int(metrics):\n",
    "    means = []\n",
    "    ses = []\n",
    "    conf_ints = []\n",
    "    metrics = np.array(metrics)\n",
    "    \n",
    "    for col in range(metrics.shape[1]):\n",
    "        mean = np.mean(metrics[:, col])\n",
    "        se = np.std(metrics[:, col], ddof = 1)/np.sqrt(metrics.shape[0])\n",
    "        t_value = stats.t.ppf(1-0.025, n_folds-1)\n",
    "        ci = [mean - t_value* se, mean + t_value * se]\n",
    "        \n",
    "        \n",
    "        means.append(mean)\n",
    "        ses.append(se)\n",
    "        conf_ints.append(ci)\n",
    "        \n",
    "    return means, ses, conf_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56708a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tn, fp, fn, tp, matthews, precision_bis, recall_bis, specificity, fnr, fpr, accuracy, f1, auc_coef, auprc, brier\n",
    "mean, se, conf_int = conf_int(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59702445",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c43572",
   "metadata": {},
   "outputs": [],
   "source": [
    "se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e678f4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tn, fp, fn, tp, matthews, precision_bis, recall_bis, specificity, fnr, fpr, accuracy, f1, auc_coef, auprc, brier\n",
    "conf_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f23f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to csv\n",
    "f = open(cv_t_test_path, \"w\")\n",
    "writer = csv.writer(f)\n",
    "metrics = np.array(metrics)\n",
    "for row in range(metrics.shape[0]):\n",
    "    writer.writerow(metrics[row])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154f200c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
