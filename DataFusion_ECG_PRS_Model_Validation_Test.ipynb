{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f67de3ee",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe57ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import gc\n",
    "import keras\n",
    "import mcfly\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Dense, concatenate\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import auc, roc_curve, precision_recall_curve, brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "from datetime import datetime \n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "import math\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6081361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose model to laod and train\n",
    "ecg_model_name = \"Model6.json\"\n",
    "prs_model_name = \"PRS1\"\n",
    "comb_idx = str(8) #iteration of combination of ECG model x and PRS model y\n",
    "\n",
    "#set to True if want to train the model (trained model will be saved), False to load trained model\n",
    "train_model = False\n",
    "\n",
    "#set to true if modify and save the PRS model architecture\n",
    "save_PRS = False \n",
    "#set to true if want to modify and save combined model architecture\n",
    "save_combined = False\n",
    "#set to true if want to save plots\n",
    "save_plots = False\n",
    "\n",
    "#set to True if want to add weights to loss function based on training class imbalance\n",
    "add_weights = False\n",
    "#set to True if want to force val to have same class imbalance as train\n",
    "balance_val = False\n",
    "\n",
    "#set number of epochs and patience\n",
    "n_epochs = 100\n",
    "n_patience = 3\n",
    "\n",
    "\"\"\"\n",
    "#path to file with indexes of files split into training, val and test\n",
    "split_path = \"Removed_no_genetic/Final_Indep_Data/split/train_val_test.json\"\n",
    "\n",
    "#paths to the labels and the data\n",
    "labels_path = \"Removed_no_genetic/Final_Indep_Data/labels/labels.npy\"\n",
    "samples_path = \"Removed_no_genetic/Final_Indep_Data/samples/\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#path to file with indexes of files split into training, val and test\n",
    "split_path = \"Removed_no_genetic/Data_Final_Same_Test_Balanced_Val/split/train_val_test.json\"\n",
    "\n",
    "#paths to the labels and the data\n",
    "labels_path = \"Removed_no_genetic/Data_Final_Same_Test_Balanced_Val/labels/labels.npy\"\n",
    "samples_path = \"Removed_no_genetic/Data_Final_Same_Test_Balanced_Val/samples/\"\n",
    "\n",
    "\n",
    "#paths to store and retrieve model types, architectures \n",
    "archi_path = \"Removed_no_genetic/Data_Fusion/architecture/architecture_\"\n",
    "type_path = \"Removed_no_genetic/Data_Fusion/type/type_\"\n",
    "\n",
    "#path to store trained model\n",
    "combined_model_name = ecg_model_name.split(\".\", 1)[0] + \"_\"+ prs_model_name + \"_\"+ comb_idx + \".json\"\n",
    "trained_path = \"Removed_no_genetic/Data_Fusion/trained/\" + combined_model_name\n",
    "\n",
    "#path to save plots\n",
    "plot_path = \"Removed_no_genetic/Data_Fusion/plots/\" + combined_model_name.split(\".\", 1)[0] + \"/\"\n",
    "test_plot_path = \"Removed_no_genetic/Data_Fusion/test_plots/\" + combined_model_name.split(\".\", 1)[0] + \"/\"\n",
    "\n",
    "#path to BRS PRS\n",
    "genetic_data_path = \"GeneticData/available_BRS_PRS.txt\"\n",
    "\n",
    "#set the seed \n",
    "random.seed(0) #generation of train, val, test sets\n",
    "np.random.seed(0) #mcfly models\n",
    "tf.random.set_seed(0) \n",
    "\n",
    "if not os.path.exists(plot_path):\n",
    "    os.makedirs(plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e69164d",
   "metadata": {},
   "source": [
    "# Dictionary with sample id and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce798887",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_array = np.load(labels_path)\n",
    "labels = dict()\n",
    "\n",
    "for row in labels_array:\n",
    "    labels[row[0]] = int(row[1])\n",
    "\n",
    "    \n",
    "del labels_array\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b962026",
   "metadata": {},
   "source": [
    "#  PRS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dcb01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRS = pd.read_csv(genetic_data_path, header=0, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb20ae0",
   "metadata": {},
   "source": [
    "# Train, val, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66a6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to open dictionary\n",
    "with open(split_path, \"r\") as fp:\n",
    "    train_val_test_dict = json.load(fp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf35c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting PRS statistics for normalisation\n",
    "train_ecg_ids = train_val_test_dict[\"train\"]\n",
    "train_ids = [int(elem.split(\"_\")[0]) for elem in train_ecg_ids]\n",
    "train_ids = list(dict.fromkeys(train_ids))\n",
    "PRS_train_for_norm = [PRS[PRS[\"anonymous_id\"] == p_id][\"SCORE\"] for p_id in train_ids] \n",
    "train_PRS_mean = np.mean(PRS_train_for_norm)\n",
    "train_PRS_std = np.std(PRS_train_for_norm, ddof=0)\n",
    "\n",
    "print(\"mean train PRS: \", train_PRS_mean , \", standard dev of train PRS: \", train_PRS_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84331db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create validation set and store in memory\n",
    "def set_generation(val_or_test, train_val_test_dict, labels, PRS, train_PRS_mean, train_PRS_std, dim = (2500, 8)):\n",
    "    n_samples = len(train_val_test_dict[val_or_test])\n",
    "\n",
    "    #Initialise\n",
    "    X_ECG = np.empty((n_samples, dim[0], dim[1]))\n",
    "    X_PRS = np.empty((n_samples), dtype = float)\n",
    "    y = np.empty((n_samples), dtype = int)\n",
    "\n",
    "    #Generate data\n",
    "    for i, ID in enumerate(train_val_test_dict[val_or_test]):\n",
    "        p_id = int(ID.split(\"_\", 1)[0])\n",
    "        \n",
    "        #store ECG sample\n",
    "        X_ECG[i,] = np.load(samples_path + ID +\".npy\")\n",
    "        \n",
    "        #store PRS sample\n",
    "        #X_PRS[i] = PRS[PRS[\"anonymous_id\"] == p_id][\"SCORE\"]\n",
    "        #X_PRS[i] = (PRS[PRS[\"anonymous_id\"] == p_id][\"SCORE\"] - train_PRS_mean) /train_PRS_std\n",
    "        X_PRS[i] = round(((PRS[PRS[\"anonymous_id\"] == p_id][\"SCORE\"] - train_PRS_mean) /train_PRS_std)*2)/2\n",
    "\n",
    "        #store class\n",
    "        y[i] = labels[ID]\n",
    "    \n",
    "    return X_ECG, X_PRS, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cde50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_ECG, X_val_PRS, y_val = set_generation(\"val\", train_val_test_dict, labels, PRS, train_PRS_mean, train_PRS_std, (2500, 8))\n",
    "y_val = keras.utils.to_categorical(y_val, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3d74f8",
   "metadata": {},
   "source": [
    "# Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0957461",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train samples: \", len(train_val_test_dict[\"train\"]))\n",
    "print(\"val samples: \", len(train_val_test_dict[\"val\"]))\n",
    "print(\"test samples: \", len(train_val_test_dict[\"test\"]))\n",
    "print(\"total samples: \", len(train_val_test_dict[\"train\"]) + len(train_val_test_dict[\"val\"]) + len(train_val_test_dict[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b533fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data proportions\n",
    "n_train = len(train_val_test_dict[\"train\"])\n",
    "n_val = len(train_val_test_dict[\"val\"])\n",
    "n_test = len(train_val_test_dict[\"test\"])\n",
    "n_tot = n_train + n_val + n_test\n",
    "print(\"proportion of train, val, test\")\n",
    "print(n_train*100/n_tot, n_val*100/n_tot, n_test*100/n_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df70b577",
   "metadata": {},
   "source": [
    "# Match val class imbalance to train's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0861566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_imbalance(data):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    \n",
    "    for elem in data: \n",
    "        if elem[0] == str(1):\n",
    "            neg = neg + 1\n",
    "        if elem[0] == str(2):\n",
    "            pos = pos +1\n",
    "    \n",
    "    if pos > neg:\n",
    "        return neg, pos, pos/neg\n",
    "            \n",
    "    return neg, pos, neg/pos   \n",
    "\n",
    "neg_train, pos_train, imb_train = calculate_imbalance(train_val_test_dict[\"train\"])\n",
    "neg_val, pos_val, imb_val = calculate_imbalance(train_val_test_dict[\"val\"])\n",
    "\n",
    "\n",
    "if balance_val: \n",
    "    if neg_train >= pos_train:\n",
    "        n_remove_from_val = round(neg_val - imb_train * pos_val)       \n",
    "        val_majority = [i for i in train_val_test_dict[\"val\"] if i.startswith('1')]   \n",
    "    else: \n",
    "        n_remove_from_val = round(pos_val - imb_train * neg_val)        \n",
    "        val_majority = [i for i in train_val_test_dict[\"val\"] if i.startswith('2')]        \n",
    "    \n",
    "    removed_samples =  random.sample(list(val_majority), n_remove_from_val)\n",
    "    train_val_test_dict[\"val\"]  = list(set(train_val_test_dict[\"val\"]).symmetric_difference(removed_samples))\n",
    "                                     \n",
    "    print(\"Remaining samples in new val: \", len(train_val_test_dict[\"val\"]))                                 \n",
    "    print(\"number of samples in new val majority class, number of samples in new val minority class, class imbalance: \")\n",
    "    print(calculate_imbalance(train_val_test_dict[\"val\"]))\n",
    "    \n",
    "    #new data proportions\n",
    "    n_train = len(train_val_test_dict[\"train\"])\n",
    "    n_val = len(train_val_test_dict[\"val\"])\n",
    "    n_test = len(train_val_test_dict[\"test\"])\n",
    "    n_tot = n_train + n_val + n_test\n",
    "    \n",
    "    print(\"proportion of data in train, new val, test: \")\n",
    "    print(n_train*100/n_tot, n_val*100/n_tot, n_test*100/n_tot)\n",
    "    \n",
    "    X_val_ECG, X_val_PRS, y_val = set_generation(\"val\", train_val_test_dict, labels, PRS, train_PRS_mean, train_PRS_std, (2500, 8)) \n",
    "    y_val = keras.utils.to_categorical(y_val, 2)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d92de56",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):    \n",
    "\n",
    "    def __init__(self, list_IDs, labels, train_PRS_mean, train_PRS_std, batch_size = 32, dim_ECG = (2500,), n_channels_ECG = 8, dim_PRS = (1,), n_channels_PRS = 1, n_classes=2, shuffle = True, seed = None):\n",
    "        #\"Initialization\"\n",
    "        self.dim_ECG = dim_ECG\n",
    "        self.dim_PRS = dim_PRS\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.train_PRS_mean = train_PRS_mean\n",
    "        self.train_PRS_std = train_PRS_std        \n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels_ECG = n_channels_ECG\n",
    "        self.n_channels_PRS = n_channels_PRS\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "        self.on_epoch_end()      \n",
    "        \n",
    "    def __len__(self):\n",
    "        #number of batches per epoch\n",
    "        return int(np.floor(len(self.list_IDs)/self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Generates indexes of one batch of data\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        #find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        \n",
    "        #Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)        \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        #updates indexes after each epoch\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            self.rng.shuffle(self.indexes)\n",
    "    \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        #Generates data containing batch_size samples\n",
    "        \n",
    "        #Initialise\n",
    "        \n",
    "        X_ECG = np.empty((self.batch_size, *self.dim_ECG, self.n_channels_ECG))\n",
    "        \n",
    "        if self.n_channels_PRS > 1:\n",
    "            X_PRS = np.empty((self.batch_size, *self.dim_PRS, self.n_channels_PRS))\n",
    "        else:  \n",
    "            X_PRS = np.empty((self.batch_size, *self.dim_PRS))\n",
    "        \n",
    "        y = np.empty((self.batch_size), dtype = int)\n",
    "        \n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            #store sample\n",
    "            X_ECG[i,] = np.load(samples_path + ID +\".npy\")\n",
    "            \n",
    "            p_id = int(ID.split(\"_\", 1)[0])  \n",
    "            \n",
    "            if self.n_channels_PRS > 1:\n",
    "                #X_PRS[i,] = PRS[PRS[\"anonymous_id\"] == p_id][\"SCORE\"]\n",
    "                #X_PRS[i,] = (PRS[PRS[\"anonymous_id\"] == p_id][\"SCORE\"] - train_PRS_mean) / train_PRS_std\n",
    "                X_PRS[i,] = round(((PRS[PRS[\"anonymous_id\"] == p_id][\"SCORE\"] - train_PRS_mean) / train_PRS_std)*2)/2\n",
    "                \n",
    "            else:\n",
    "                #X_PRS[i] = PRS[PRS[\"anonymous_id\"] == p_id][\"SCORE\"]\n",
    "                #X_PRS[i] = (PRS[PRS[\"anonymous_id\"] == p_id][\"SCORE\"] - train_PRS_mean) / train_PRS_std\n",
    "                X_PRS[i] = round(((PRS[PRS[\"anonymous_id\"] == p_id][\"SCORE\"] - train_PRS_mean) / train_PRS_std)*2)/2\n",
    "                \n",
    "            #store class\n",
    "            y[i] = self.labels[ID]         \n",
    "        \n",
    "                   \n",
    "        return [X_ECG, X_PRS], keras.utils.to_categorical(y, num_classes = self.n_classes)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8798ec45",
   "metadata": {},
   "source": [
    "# Load ECG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1910ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(archi_path + ecg_model_name, \"r\") as f:\n",
    "    model_loaded = json.load(f)\n",
    "    model = keras.models.model_from_json(model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215fdcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(type_path + ecg_model_name, \"r\") as f:\n",
    "    model_type = json.load(f)    \n",
    "    print(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bf34fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba1b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECG_model = Model(inputs = model.input, outputs = model.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8171c9b9",
   "metadata": {},
   "source": [
    "# Make PRS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df2687",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRS_input = Input(shape = (1,))\n",
    "x = Dense(1, activation = \"relu\")(PRS_input)\n",
    "PRS_model = Model(PRS_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd8fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_PRS: \n",
    "    with open(archi_path + prs_model_name + \".json\", \"w\") as f:\n",
    "                    json.dump(PRS_model.to_json(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba1a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(archi_path + prs_model_name + \".json\", \"r\") as f:\n",
    "    prs_loaded = json.load(f)\n",
    "    prs = keras.models.model_from_json(prs_loaded)\n",
    "    prs.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b23c754",
   "metadata": {},
   "source": [
    "# Make combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d5622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_combined: \n",
    "    combined_input = concatenate([ECG_model.output, PRS_model.output])\n",
    "    z = Dense(3, activation = \"relu\")(combined_input)\n",
    "    z = Dense(2, activation = \"softmax\")(z)\n",
    "    combined_model = Model(inputs = [ECG_model.inputs, PRS_model.input], outputs = z)\n",
    "    \n",
    "    with open(archi_path + combined_model_name, \"w\") as f:\n",
    "                    json.dump(combined_model.to_json(), f)\n",
    "            \n",
    "    plot_model(combined_model, to_file = plot_path + \"combined_model.png\", show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fe02e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(archi_path + combined_model_name, \"r\") as f:\n",
    "    comb_loaded = json.load(f)\n",
    "    combined_model = keras.models.model_from_json(comb_loaded)\n",
    "    combined_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52ac6e",
   "metadata": {},
   "source": [
    "# Train / Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c372553",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model: \n",
    "    #define parameters\n",
    "    params = {\"dim_ECG\" : (2500,),\n",
    "              \"dim_PRS\" : (1,),\n",
    "             \"batch_size\": 32,\n",
    "             \"n_classes\": 2,\n",
    "             \"n_channels_ECG\":8,\n",
    "             \"n_channels_PRS\":1,\n",
    "             \"shuffle\" :True,\n",
    "             \"seed\": 0}\n",
    "     \n",
    "    \n",
    "    #Generators \n",
    "    training_generator = DataGenerator(train_val_test_dict[\"train\"], labels, train_PRS_mean, train_PRS_std, **params)  \n",
    "\n",
    "    metric = [\"accuracy\"]\n",
    "    combined_model.compile(loss=\"categorical_crossentropy\", optimizer = \"adam\", metrics = metric)\n",
    "    \n",
    "    if add_weights:      \n",
    "\n",
    "        #calculate class imbalance\n",
    "        zeroes = 0\n",
    "        ones = 0\n",
    "        for i, ID in enumerate(train_val_test_dict[\"train\"]):\n",
    "            if labels[ID] == 0:\n",
    "                zeroes = zeroes + 1\n",
    "            if labels[ID] == 1:\n",
    "                ones = ones + 1\n",
    "\n",
    "        if ones < zeroes:\n",
    "            class_weights = {0: 1., 1: zeroes/ones}\n",
    "        elif zeroes < ones:\n",
    "            class_weights = {0: ones/zeroes, 1: 1.}\n",
    "        else:\n",
    "            class_weights = {0: 1., 1: 1.}\n",
    "    else:\n",
    "        class_weights = {0: 1., 1: 1.}\n",
    "        \n",
    "    #print time    \n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(current_time)\n",
    "    \n",
    "    #train\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = n_patience, restore_best_weights = True)\n",
    "    history = combined_model.fit(training_generator,\n",
    "              validation_data = ([X_val_ECG, X_val_PRS ], y_val), \n",
    "              epochs = n_epochs,\n",
    "              class_weight = class_weights, \n",
    "              callbacks = callback,\n",
    "              verbose = True)\n",
    "    \n",
    "    #print time\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(current_time)\n",
    "    \n",
    "    #save the model\n",
    "    combined_model.save(trained_path)\n",
    "       \n",
    "else:\n",
    "    #load the model\n",
    "    combined_model = keras.models.load_model(trained_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c631b",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d084181",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probas = combined_model.predict([X_val_ECG, X_val_PRS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no BrS would appear as 0, hence transformed to [1,0] => the first column returns 1 if no BrS, 0 otherwise\n",
    "no_BrS = y_val[:, 0]\n",
    "\n",
    "#BrS appears as 1, hence transformed to [0,1] => the second column returns 1 if BrS, 0 otherwise\n",
    "BrS = y_val[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f1b450",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85cbc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BrS_probas = pred_probas[:,1]\n",
    "BrS_predictions = pred_probas.argmax(axis = -1)\n",
    "BrS_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c832d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metrics(y_true, y_pred, y_proba):\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion matrix: \")\n",
    "    print(conf_mat)\n",
    "    tn,fp,fn,tp = conf_mat.ravel()\n",
    "    print(\"tn: \", tn,\" fp: \", fp,\" fn: \", fn,\" tp: \", tp)\n",
    "    \n",
    "    print(\"\")\n",
    "    matthews = ((tp*tn) - (fp*fn)) / math.sqrt(((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "    print(\"Matthews Correlation Coefficient: \", matthews)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    print(\"\")           \n",
    "    precision_bis = tp/(tp+fp) #positive predictive value\n",
    "    recall_bis = tp/(tp+fn)\n",
    "    f1 = 2*precision_bis*recall_bis/(precision_bis+recall_bis)\n",
    "    specificity = tn/(tn+fp) #true negative rate\n",
    "    fnr = fn/(fn+tp)\n",
    "    FPR = fp/(fp+tn)\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "    \n",
    "    print(\"precision/positive predictive value: \", precision_bis)\n",
    "    print(\"recall/sensitivity: \", recall_bis)\n",
    "    print(\"specificity/true negative rate: \", specificity)\n",
    "    print(\"False negative rate: \", fnr)\n",
    "    print(\"False positive rate: \", FPR)\n",
    "    print(\"accuracy: \", accuracy)    \n",
    "    print(\"f1 score: \", f1) \n",
    "\n",
    "      \n",
    "    print(\"\")\n",
    "    brier = brier_score_loss(y_true, y_proba)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
    "    auc_coef = auc(fpr, tpr)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    auprc = auc(recall, precision)\n",
    "    print(\"brier score: \", brier )\n",
    "    print(\"auc: \", auc_coef)\n",
    "    print(\"auprc: \", auprc)\n",
    "    \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec4074",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(BrS, BrS_predictions, BrS_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651b3cf6",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4cd997",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model:\n",
    "    #plot train and validation loss\n",
    "    training_loss = history.history[\"loss\"]\n",
    "    validation_loss = history.history[\"val_loss\"]\n",
    "\n",
    "    #number of epochs\n",
    "    epoch_count = range(1, len(training_loss) +1)\n",
    "\n",
    "    #visualise loss history\n",
    "    f, ax = plt.subplots(figsize=(6,6))      \n",
    "    ax.plot(epoch_count, training_loss, \"r--\", label=\"Training Loss\")\n",
    "    ax.plot(epoch_count, validation_loss, \"b-\", label=\"Validation Loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Training and Validation Loss Over the Epochs\")\n",
    "    ax.legend()\n",
    "    plt.savefig(plot_path + \"Loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e65fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(BrS, BrS_probas)\n",
    "auc_coef = round(auc(fpr, tpr),3)\n",
    "f, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(fpr, tpr, marker=\".\", label = model_type[\"type\"] + \" - AUC: \" + str(auc_coef))\n",
    "ax.plot([0,1], [0,1], transform = ax.transAxes, linestyle=\"--\", label=\"Random Classifier\")\n",
    "ax.set_ylim(bottom=0, top = 1)\n",
    "ax.set_xlim([0,1])\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "#ax.set_title(\"ROC\")\n",
    "ax.legend()\n",
    "if save_plots:\n",
    "    plt.savefig(plot_path + \"ROC.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a404ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision Recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(BrS, BrS_probas)\n",
    "auprc = round(auc(recall, precision),3)\n",
    "f, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(recall, precision, marker=\".\", label = model_type[\"type\"] + \" - AUPRC: \" + str(auprc))\n",
    "ax.set_xlabel(\"Recall (Positive label: Brugada)\")\n",
    "ax.set_ylabel(\"Precision (Positive label: Brugada)\")\n",
    "#ax.set_title(\"AUPRC\")\n",
    "ax.set_ylim(bottom=0, top = 1)\n",
    "ax.set_xlim([0,1])\n",
    "ax.legend()\n",
    "\n",
    "if save_plots:\n",
    "    plt.savefig(plot_path + \"PrecisionRecallCurve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfe1b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calibration\n",
    "# bin data and normalise counts\n",
    "def counts_to_percentages(probabilities):\n",
    "    bin0_01 = 0\n",
    "    bin01_02=0\n",
    "    bin02_03=0\n",
    "    bin03_04=0\n",
    "    bin04_05=0\n",
    "    bin05_06=0\n",
    "    bin06_07=0\n",
    "    bin07_08=0\n",
    "    bin08_09=0\n",
    "    bin09_1=0 \n",
    "    \n",
    "    for val in probabilities:\n",
    "    \n",
    "        if val <0.1:\n",
    "            bin0_01 = bin0_01 + 1\n",
    "    \n",
    "        elif val >= 0.1 and val <0.2:\n",
    "            bin01_02= bin01_02 +1 \n",
    "    \n",
    "        elif val >= 0.2 and val <0.3:\n",
    "            bin02_03= bin02_03 +1 \n",
    "    \n",
    "        elif val >= 0.3 and val <0.4:\n",
    "                bin03_04= bin03_04 +1\n",
    "    \n",
    "        elif val >= 0.4 and val <0.5:\n",
    "                bin04_05= bin04_05 +1 \n",
    "    \n",
    "        elif val >= 0.5 and val <0.6:\n",
    "                bin05_06= bin05_06 +1 \n",
    "    \n",
    "        elif val >= 0.6 and val <0.7:\n",
    "                    bin06_07= bin06_07 +1 \n",
    "    \n",
    "        elif val >= 0.7 and val <0.8:\n",
    "                    bin07_08= bin07_08 +1 \n",
    "    \n",
    "        elif val >= 0.8 and val <0.9:\n",
    "                    bin08_09= bin08_09 +1 \n",
    "    \n",
    "        elif val >= 0.9:\n",
    "                    bin09_1= bin09_1 +1 \n",
    "                \n",
    "    counts = [bin0_01, bin01_02, bin02_03, bin03_04, bin04_05,\n",
    "             bin05_06, bin06_07, bin07_08, bin08_09, bin09_1]    \n",
    "    \n",
    "    percentages = counts/np.sum(counts) *100\n",
    "    \n",
    "    return percentages\n",
    "    \n",
    "    \n",
    "#plot calibration plot and histogram together\n",
    "def calibration_together (BrS, BrS_probas, plot_path, per_patient = False):        \n",
    "    print(\"plot curves and save in one png file\")\n",
    "    #save three plots in one png file\n",
    "    fig_index = 1\n",
    "      \n",
    "    #save three plots in one png file\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(7, 12))   \n",
    "    \n",
    "    # plot calibration curve LSTM\n",
    "    y, x = calibration_curve(BrS, BrS_probas, n_bins=10)\n",
    "\n",
    "    ax1.plot(x, y, 'C0',marker='o', linewidth=1, label= model_type[\"type\"], color = \"darkturquoise\") \n",
    "    ax1.set(xlabel= 'Predicted score', ylabel= 'True probability in each bin')\n",
    "    \n",
    "    ax1.set_ylim(bottom=-0.2, top = 1.2)\n",
    "    ax1.set_xlim([-0.2,1.2])\n",
    "    line = mlines.Line2D([0, 1], [0, 1], color='black', linestyle='--', linewidth=0.9, label= \"Perfectly calibrated\")\n",
    "    transform = ax1.transAxes\n",
    "    line.set_transform(transform)\n",
    "    ax1.add_line(line)     \n",
    "    ax1.legend(loc=\"upper left\")  \n",
    "  \n",
    "    #HISTOGRAMS    \n",
    "    x = np.arange(0,1,0.1)\n",
    "    \n",
    "    y = counts_to_percentages(BrS_probas)   #if instead of % want values in [0,1], do: y = counts_to_percentages(proba)/100 \n",
    "    ax2.hist(x, range = [0,1], bins=10, weights = y, label= model_type[\"type\"],\n",
    "                 histtype=\"step\", lw=3.5, color = \"darkturquoise\")\n",
    "    \n",
    "    ax2.set_xlabel(\"Mean predicted score\")\n",
    "    ax2.set_ylabel(\"Percentage of counts\")\n",
    "    ax2.legend(loc=\"upper center\", ncol=5)\n",
    "    ax2.set_ylim([0,101]) #if instead of % want probabilities, change to [0,1]     \n",
    "\n",
    "    #plt.tight_layout()\n",
    "    if per_patient: \n",
    "        #plt.savefig(plot_path + \"Calibration_PP.png\")\n",
    "        print(\"hi\")\n",
    "    elif save_plots:\n",
    "        #plt.savefig(plot_path + \"Calibration.png\")\n",
    "        print(\"hi\")\n",
    "    \n",
    "    plt.show()\n",
    "        \n",
    "    return\n",
    "\n",
    "calibration_together(BrS, BrS_probas, plot_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbe441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discrimination\n",
    "def distribution(BrS, BrS_probas, plot_path, per_patient = False):\n",
    "    #probabilities distributions graphs\n",
    "    true_1 = pd.DataFrame(BrS_probas, columns=['Predicted probabilities'])\n",
    "    true_1['labels'] = BrS.tolist()\n",
    "    true_0 = true_1.copy(deep = True) \n",
    "    indexNames = true_1[true_1['labels'] == 0].index\n",
    "    true_1.drop(indexNames , inplace=True)\n",
    "    indexNames = true_0[ true_0['labels'] == 1 ].index\n",
    "    true_0.drop(indexNames , inplace=True)\n",
    "    true_1.drop(columns=['labels'], inplace = True)\n",
    "    true_0.drop(columns=['labels'], inplace = True)\n",
    "    \n",
    "    sns.distplot(true_1['Predicted probabilities'], hist = False, kde = True,\n",
    "                 kde_kws = {'shade': True, 'linewidth': 3,\"color\": \"r\"}, label = 'Class 1')\n",
    "    sns.distplot(true_0['Predicted probabilities'], hist = False, kde = True,\n",
    "                     kde_kws = {'shade': True, 'linewidth': 3, \"color\": \"g\"}, label = 'Class 0')\n",
    "    plt.ylabel('Density')\n",
    "    plt.xlabel('Predicted score')\n",
    "    plt.legend(labels=[\"BrP\",\"No BrP\"])\n",
    "\n",
    "    if per_patient: \n",
    "        #plt.savefig(plot_path + \"Discrimiation_PP.png\")\n",
    "        print(\"hi\")\n",
    "    elif save_plots:\n",
    "        #plt.savefig(plot_path + \"Discrimination.png\")\n",
    "        print(\"hi\")\n",
    "        \n",
    "    plt.show()\n",
    "    plt.clf()    \n",
    "    return\n",
    "\n",
    "distribution(BrS, BrS_probas, plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6970b642",
   "metadata": {},
   "source": [
    "# Check if overfitting on the PRS\n",
    "## Check if predictions that were right/wrong came from patients that were also on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde40e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p_ids = [int(ID.split(\"_\", 1)[0]) for ID in train_val_test_dict[\"train\"]]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d872483",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p_ids_counts = Counter(train_p_ids) #number of occurences of a patient id in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe3e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_in_train = []\n",
    "val_ids = train_val_test_dict[\"val\"]\n",
    "\n",
    "for ID in val_ids:   \n",
    "    #get data\n",
    "    p_id = int(ID.split(\"_\", 1)[0])\n",
    "    mini_x_ecg = np.load(samples_path + ID +\".npy\")\n",
    "    #mini_x_prs = PRS[PRS[\"anonymous_id\"] == p_id][\"SCORE\"]\n",
    "    #mini_x_prs = (PRS[PRS[\"anonymous_id\"] == p_id][\"SCORE\"] - train_PRS_mean) / train_PRS_std\n",
    "    mini_x_prs = round(((PRS[PRS[\"anonymous_id\"] == p_id][\"SCORE\"] - train_PRS_mean) /train_PRS_std)*2)/2\n",
    "    \n",
    "    \n",
    "    mini_y = labels[ID]\n",
    "    \n",
    "    \n",
    "    #predict      \n",
    "    mini_pred_probas = combined_model.predict([np.expand_dims(mini_x_ecg, 0), mini_x_prs])\n",
    "\n",
    "    #True label\n",
    "    mini_BrS = mini_y\n",
    "\n",
    "    #get probabilities and predictions\n",
    "    mini_BrS_probas = mini_pred_probas[:,1]\n",
    "    mini_BrS_predictions = mini_pred_probas.argmax(axis = -1)\n",
    "    \n",
    "    if mini_BrS == int(mini_BrS_predictions):\n",
    "        correct_prediction = True\n",
    "    else: \n",
    "        correct_prediction = False\n",
    "        \n",
    "    print(mini_BrS, mini_BrS_predictions, correct_prediction)\n",
    "    \n",
    "    if p_id in train_p_ids_counts: \n",
    "        row = [ID, True, train_p_ids_counts[p_id], correct_prediction]\n",
    "    else: \n",
    "        row = [ID, False, 0, correct_prediction]\n",
    "    \n",
    "    patient_in_train.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e71cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_in_train = pd.DataFrame(patient_in_train, columns = [\"ECG_id\", \"patient_in_train\", \"patient_samples_in_train\", \"correct_prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1914f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check number of TT/tot in train, TF/tot in train, FT/tot not in train, FF//tot not in train\n",
    "TT = patient_in_train[patient_in_train['patient_in_train'] & patient_in_train['correct_prediction']].shape[0]\n",
    "TF = patient_in_train[patient_in_train['patient_in_train'] & (patient_in_train['correct_prediction'] == False)].shape[0]\n",
    "FT = patient_in_train[(patient_in_train['patient_in_train'] == False) & patient_in_train['correct_prediction']].shape[0]\n",
    "FF = patient_in_train[~(patient_in_train['patient_in_train'] | patient_in_train['correct_prediction'])].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df6803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TT+TF+FT+FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcd7099",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_train = TT + TF\n",
    "not_in_train = FT + FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120829ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"proportion of val samples from patients found in train that were correctly classified: \", TT/(in_train+0.0000000001))\n",
    "print(\"proportion of val samples from patients found in train that were incorrectly classified: \", TF/(in_train+0.0000000001))\n",
    "print(\"proportion of val samples from patients NOT found in train that were correctly classified: \", FT/not_in_train)\n",
    "print(\"proportion of val samples from patients NOT found in train that were incorrectly classified: \", FF/not_in_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58d40c2",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd40a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ECG, X_test_PRS, y_test = set_generation(\"test\", train_val_test_dict, labels, PRS, train_PRS_mean, train_PRS_std, (2500, 8))\n",
    "y_test = keras.utils.to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d723e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probas = combined_model.predict([X_test_ECG, X_test_PRS])\n",
    "#BrS appears as 1, hence transformed to [0,1] => the second column returns 1 if BrS, 0 otherwise\n",
    "BrS = y_test[:,1]\n",
    "BrS_probas = pred_probas[:,1]\n",
    "BrS_predictions = pred_probas.argmax(axis = -1)\n",
    "\n",
    "performance_metrics(BrS, BrS_predictions, BrS_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab463e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(BrS, BrS_probas)\n",
    "auc_coef = round(auc(fpr, tpr),3)\n",
    "f, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(fpr, tpr, marker=\".\", label = model_type[\"type\"] + \" - AUC: \" + str(auc_coef))\n",
    "ax.plot([0,1], [0,1], transform = ax.transAxes, linestyle=\"--\", label=\"Random Classifier\")\n",
    "ax.set_ylim(bottom=0, top = 1)\n",
    "ax.set_xlim([0,1])\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "#ax.set_title(\"ROC\")\n",
    "ax.legend()\n",
    "if save_plots:\n",
    "    plt.savefig(test_plot_path + \"ROC.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fecbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision Recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(BrS, BrS_probas)\n",
    "auprc = round(auc(recall, precision),3)\n",
    "f, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(recall, precision, marker=\".\", label = model_type[\"type\"] + \" - AUPRC: \" + str(auprc))\n",
    "ax.set_xlabel(\"Recall (Positive label: Brugada)\")\n",
    "ax.set_ylabel(\"Precision (Positive label: Brugada)\")\n",
    "#ax.set_title(\"AUPRC\")\n",
    "ax.set_ylim(bottom=0, top = 1)\n",
    "ax.set_xlim([0,1])\n",
    "ax.legend()\n",
    "\n",
    "if save_plots:\n",
    "    plt.savefig(test_plot_path + \"PrecisionRecallCurve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f2072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_together(BrS, BrS_probas, test_plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution(BrS, BrS_probas, test_plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d4b51",
   "metadata": {},
   "source": [
    "# Testing Per Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12b3abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train_val_test_dict[\"test\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped= []\n",
    "for p in test:\n",
    "     stripped.append(p.split(\"_\", 1)[0]) #remove everythin after \"_\"\n",
    "stripped = list(dict.fromkeys(stripped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e47f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(test)\n",
    "test_df.columns = [\"ecg_id\"]\n",
    "test_df = pd.Series(test_df.ecg_id) \n",
    "dim = (2500, 8)\n",
    "mini_ecg_x = np.empty((1, dim[0], dim[1]))\n",
    "mini_prs_x = np.empty((1,1))\n",
    "\n",
    "\n",
    "file_id_conf_mat = {\"TN\":[], \"TP\":[], \"FN\": [], \"FP\":[]}\n",
    "p_id_reprod = {}\n",
    "\n",
    "for p in stripped:\n",
    "    all_samples =  list(test_df.loc[test_df.str.contains(p)].values)\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    \n",
    "    for s in all_samples:\n",
    "        \n",
    "        mini_ecg_x[0,] = np.load(samples_path + s +\".npy\")\n",
    "        mini_prs_x[0,] = round(((PRS[PRS[\"anonymous_id\"] == int(p)][\"SCORE\"] - train_PRS_mean) /train_PRS_std)*2)/2\n",
    "        \n",
    "        mini_y = labels[s] \n",
    "        if mini_y == 0: \n",
    "            mini_y = [1,0]\n",
    "        if mini_y == 1:\n",
    "            mini_y = [0,1]\n",
    "   \n",
    "        #predict and get performance metrics    \n",
    "        mini_pred_probas = combined_model.predict([mini_ecg_x, mini_prs_x])\n",
    "\n",
    "        #BrS appears as 1, hence transformed to [0,1] => the second column returns 1 if BrS, 0 otherwise\n",
    "        mini_BrS = mini_y[1]\n",
    "\n",
    "        #get probabilities and predictions\n",
    "        mini_BrS_probas = mini_pred_probas[:,1]\n",
    "        mini_BrS_predictions = mini_pred_probas.argmax(axis = -1)\n",
    "        \n",
    "        \n",
    "        if mini_BrS == 0:\n",
    "            if mini_BrS_predictions == 0:\n",
    "                TN = TN +1\n",
    "                file_id_conf_mat[\"TN\"].append(s)\n",
    "                \n",
    "                \n",
    "            if mini_BrS_predictions == 1:\n",
    "                FP = FP +1\n",
    "                file_id_conf_mat[\"FP\"].append(s)\n",
    "        \n",
    "        if mini_BrS == 1:\n",
    "            if mini_BrS_predictions == 1:\n",
    "                TP = TP +1\n",
    "                file_id_conf_mat[\"TP\"].append(s)\n",
    "                \n",
    "            if mini_BrS_predictions == 0:\n",
    "                FN = FN +1\n",
    "                file_id_conf_mat[\"FN\"].append(s)\n",
    "                \n",
    "    p_id_reprod[p] = [labels[s], TN, FP, TP, FN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadac338",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = p_id_reprod\n",
    "reprod = pd.DataFrame.from_dict(data, orient='index',\n",
    "                       columns=['label', 'TN', 'FP', 'TP', 'FN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02ee13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reprod[\"number_of_samples\"] = reprod[\"TN\"] + reprod[\"TP\"] + reprod[\"FN\"] + reprod[\"FP\"]\n",
    "reprod[\"fraction_correct_labels\"] = (reprod[\"TN\"] + reprod[\"TP\"]) / reprod[\"number_of_samples\"]\n",
    "reprod[\"all_samples_correctly_predicted\"] = np.where(reprod[\"fraction_correct_labels\"]== 1, True, False)\n",
    "reprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583db9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of different negative test patients: \", reprod[reprod[\"label\"]==0].shape[0], \" number of negative samples: \",sum(reprod[reprod[\"label\"]==0][\"number_of_samples\"])) \n",
    "print(\"number negative samples per patient: \", sum(reprod[reprod[\"label\"]==0][\"number_of_samples\"])/reprod[reprod[\"label\"]==0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6058ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of different positive test patients: \", reprod[reprod[\"label\"]==1].shape[0], \" number of positive samples: \",sum(reprod[reprod[\"label\"]==1][\"number_of_samples\"]))\n",
    "print(\"number positive samples per patient: \", sum(reprod[reprod[\"label\"]==1][\"number_of_samples\"])/reprod[reprod[\"label\"]==1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0343fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of samples for positive patients\")\n",
    "sum(reprod[reprod[\"label\"]==1][\"number_of_samples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d8a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of samples for negative patients\")\n",
    "sum(reprod[reprod[\"label\"]==0][\"number_of_samples\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94ea0a7",
   "metadata": {},
   "source": [
    "## Drop patients with less than two samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a48ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reprod = reprod[reprod[\"number_of_samples\"]>=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a98eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of different negative test patients: \", reprod[reprod[\"label\"]==0].shape[0], \" number of negative samples: \",sum(reprod[reprod[\"label\"]==0][\"number_of_samples\"])) \n",
    "print(\"number negative samples per patient: \", sum(reprod[reprod[\"label\"]==0][\"number_of_samples\"])/reprod[reprod[\"label\"]==0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e957f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of different positive test patients: \", reprod[reprod[\"label\"]==1].shape[0], \" number of positive samples: \",sum(reprod[reprod[\"label\"]==1][\"number_of_samples\"]))\n",
    "print(\"number positive samples per patient: \", sum(reprod[reprod[\"label\"]==1][\"number_of_samples\"])/reprod[reprod[\"label\"]==1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ab375",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of samples for positive patients\")\n",
    "sum(reprod[reprod[\"label\"]==1][\"number_of_samples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80302168",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of samples for negative patients\")\n",
    "sum(reprod[reprod[\"label\"]==0][\"number_of_samples\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629c49f7",
   "metadata": {},
   "source": [
    "### Fraction of correct labels : within patient agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e582c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"General fraction of correct labels\")\n",
    "print(np.mean(reprod[\"fraction_correct_labels\"]), np.median(reprod[\"fraction_correct_labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c5f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(10, 7))\n",
    "ax = fig.add_axes([0, 0, 1, 1]) \n",
    "bp = ax.boxplot(reprod[\"fraction_correct_labels\"]) \n",
    "ax.set_xticklabels(['All groups'])\n",
    "plt.title(\"Distribution of fraction of correct labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4e3cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = reprod[reprod[\"label\"]==0][\"fraction_correct_labels\"]\n",
    "data_2 = reprod[reprod[\"label\"]==1][\"fraction_correct_labels\"]\n",
    "df = [data_1, data_2]\n",
    "fig = plt.figure(figsize =(10, 7)) \n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.set_xticklabels(['No BrP', 'BrP'])\n",
    "bp = ax.boxplot(df)\n",
    "plt.title(\"Distribution of fraction of correct labels for positive and negative samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d9f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"mean ratio of correct predictions per patient, positives: \", np.mean(reprod[reprod[\"label\"]==1][\"fraction_correct_labels\"]),\n",
    "     \", negatives: \", np.mean(reprod[reprod[\"label\"]==0][\"fraction_correct_labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e776ee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of samples per patient for patients for which at least one prediction was wrong\")\n",
    "print(np.mean(reprod[reprod[\"all_samples_correctly_predicted\"]==False][\"number_of_samples\"]), \n",
    "      np.median(reprod[reprod[\"all_samples_correctly_predicted\"]==False][\"number_of_samples\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26071af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of samples per patient for patients for which all predictions were right\")\n",
    "print(np.mean(reprod[reprod[\"all_samples_correctly_predicted\"]==True][\"number_of_samples\"]), \n",
    "      np.median(reprod[reprod[\"all_samples_correctly_predicted\"]==True][\"number_of_samples\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd30bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = reprod[reprod[\"all_samples_correctly_predicted\"]==True] #patients for which 100% within patient agreement was obtained\n",
    "incorrect = reprod[reprod[\"all_samples_correctly_predicted\"]==False] #patients for which less than 100% within patient agreement was obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"propotion of samples that reached 100% within patient agreement \", correct.shape[0]/(correct.shape[0]+incorrect.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfbc76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplots\n",
    "data_1 = correct[correct[\"label\"]==1][\"number_of_samples\"]\n",
    "data_2 = correct[correct[\"label\"]==0][\"number_of_samples\"]\n",
    "data_3 = incorrect[incorrect[\"label\"]==1][\"number_of_samples\"]\n",
    "data_4 = incorrect[incorrect[\"label\"]==0][\"number_of_samples\"]\n",
    "\n",
    "df = [data_1, data_2, data_3, data_4]\n",
    "fig = plt.figure(figsize=(10,8)) \n",
    "ax = fig.add_axes([0.1, 0.1, 0.75, 0.75])\n",
    "ax.set_xticklabels([\"All correct and BrP\", \"All correct and no BrP\", \">1 incorrect and BrP\", \">1 incorrect and no BrP\"])\n",
    "ax.set_ylabel(\"Number of samples per patient \")\n",
    "ax.boxplot(df)\n",
    "#plt.title(\"Distribution of number of samples with respect to whether all samples were correctly classified for one patient per true label\")\n",
    "#fig.savefig(test_plot_path + \"BoxPlot_all_correct_at_least_one_wrong_per_class.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f289876c",
   "metadata": {},
   "source": [
    "### New AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c2c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(figsize = (10,10))\n",
    "colors = {0:\"green\", 1: \"red\"}\n",
    "labels = {0: \"no BrP\", 1: \"BrP\"}\n",
    "grouped = reprod.groupby(\"label\")\n",
    "for key, group in grouped:\n",
    "    group.plot(ax = ax, kind =\"scatter\", x = \"number_of_samples\", y= \"fraction_correct_labels\", label = labels[key], color = colors[key], s=50)\n",
    "ax.set(xlabel = \"Samples available per patient\", ylabel = \"Fraction of correctly predicted labels\")\n",
    "plt.rc(\"axes\", labelsize=20)\n",
    "plt.rc(\"legend\", fontsize=20)\n",
    "plt.rc(\"xtick\", labelsize = 20)\n",
    "plt.rc(\"ytick\", labelsize = 20)\n",
    "plt.savefig(test_plot_path + \"scatter_fraction_correct_per_n_samples.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5355dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new probabilities\n",
    "#if positive label probability of BrP is fraction of correct\n",
    "#if negative label, probability of BrP is 1- fraction of correct\n",
    "reprod[\"new_probas\"] = \"\"\n",
    "reprod.loc[reprod[\"label\"]==1, \"new_probas\"] = reprod[\"fraction_correct_labels\"]\n",
    "reprod.loc[reprod[\"label\"]==0, \"new_probas\"] = 1 - reprod[\"fraction_correct_labels\"]\n",
    "reprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c54dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict based on fraction of correctly predicted\n",
    "original_labels = reprod[\"label\"]\n",
    "new_probas = reprod[\"new_probas\"]\n",
    "\n",
    "new_predictions = [1 if elem >= 0.5 else 0 for elem in new_probas]\n",
    "#new_predictions = reprod[\"new_probas\"].astype(\"float\").round(0)\n",
    "metrics =[]\n",
    "performance_metrics(original_labels,new_predictions, new_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f958dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(original_labels, new_probas)\n",
    "auc_coef = round(auc(fpr, tpr),3)\n",
    "f, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(fpr, tpr, marker=\".\", label = model_type[\"type\"] + \" - AUC: \" + str(auc_coef))\n",
    "ax.plot([0,1], [0,1], transform = ax.transAxes, linestyle=\"--\", label=\"Random Classifier\")\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "#ax.set_title(\"ROC\")\n",
    "ax.legend()\n",
    "#plt.savefig(test_plot_path + \"ROC_PP.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc1fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision Recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(original_labels, new_probas)\n",
    "auprc = round(auc(recall, precision),3)\n",
    "f, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(recall, precision, marker=\".\", label = model_type[\"type\"] + \" - AUPRC: \" + str(auprc))\n",
    "ax.set_xlabel(\"Recall (Positive label: Brugada)\")\n",
    "ax.set_ylabel(\"Precision (Positive label: Brugada)\")\n",
    "#ax.set_title(\"AUPRC\")\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.legend()\n",
    "#plt.savefig(test_plot_path + \"AUPRC_PP.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9aae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_together(original_labels.astype(float), new_probas.astype(float), test_plot_path, per_patient = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45dd4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution(original_labels, {\"Predicted probabilities\": np.array(new_probas)}, test_plot_path, per_patient = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ceeb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export predictions to csv\n",
    "#labels_and_predictions_p_sample = pd.DataFrame(list(zip(BrS, BrS_probas)), columns = [\"label_per_sample\", \"prediction_per_sample\"])\n",
    "#labels_and_predictions_p_patient = pd.DataFrame(list(zip(original_labels, new_probas)), columns = [\"label_per_sample\", \"prediction_per_sample\"])\n",
    "#labels_and_predictions_p_sample.to_csv(\"ecg_prs_predictions_per_sample.csv\", index=False)\n",
    "#labels_and_predictions_p_patient.to_csv(\"ecg_prs_predictions_per_patient.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
