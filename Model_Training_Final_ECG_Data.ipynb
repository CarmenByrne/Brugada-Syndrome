{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f67de3ee",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe57ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import gc\n",
    "import keras\n",
    "import mcfly\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import auc, roc_curve, precision_recall_curve\n",
    "from sklearn.calibration import calibration_curve\n",
    "from datetime import datetime \n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "import math\n",
    "from lime import explanation\n",
    "from lime import lime_base\n",
    "from lime_timeseries import LimeTimeSeriesExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6081361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set to True if want to generate models, if already ran the script and have the models set to False\n",
    "generate_models = False\n",
    "#number of models to generate, if generate_models = True\n",
    "n_models = 10\n",
    "\n",
    "#set to True if want to train the model, False to load trained model\n",
    "train_model = False\n",
    "#set to True if want to save the plots\n",
    "save_plots = False\n",
    "\n",
    "#set to True if want to add weights to loss function\n",
    "add_weights = True\n",
    "\n",
    "\n",
    "#path to file with indexes of files split into training, val and test\n",
    "#split_path = \"400_dumped/Final_Data/split/train_val_test.json\"\n",
    "split_path = \"400_to_val/Final_Data/split/train_val_test.json\"\n",
    "\n",
    "#paths to the labels and the data\n",
    "#labels_path = \"400_dumped/Final_Data/labels/labels.npy\"\n",
    "#samples_path = \"400_dumped/Final_Data/samples/\"\n",
    "\n",
    "labels_path = \"400_to_val/Final_Data/labels/labels.npy\"\n",
    "samples_path = \"400_to_val/Final_Data/samples/\"\n",
    "\n",
    "#paths to store and retrieve model types, architectures and hyperparameters\n",
    "#archi_path = \"400_dumped/Models_Final_Data/architecture/architecture_\"\n",
    "#params_path = \"400_dumped/Models_Final_Data/parameters/params_\"\n",
    "#type_path = \"400_dumped/Models_Final_Data/type/type_\"\n",
    "\n",
    "archi_path = \"400_to_val/Models_Final_Data/architecture/architecture_\"\n",
    "params_path = \"400_to_val/Models_Final_Data/parameters/params_\"\n",
    "type_path = \"400_to_val/Models_Final_Data/type/type_\"\n",
    "\n",
    "#choose model to laod and train\n",
    "model_name = \"Model0.json\"\n",
    "\n",
    "#path to store trained model\n",
    "#trained_path = \"400_dumped/Models_Final_Data/trained/\" + model_name\n",
    "trained_path = \"400_to_val/Models_Final_Data/trained/\" + model_name\n",
    "\n",
    "#path to save plots\n",
    "model_name_no_extension = model_name.split(\".\", 1)[0] + \"/\"\n",
    "#plot_path = \"400_dumped/Models_Final_Data/plots/\" + model_name_no_extension\n",
    "plot_path = \"400_to_val/Models_Final_Data/plots/\" + model_name_no_extension\n",
    "\n",
    "#set the seed \n",
    "random.seed(0) #generation of train, val, test sets\n",
    "np.random.seed(0) #mcfly models\n",
    "tf.random.set_seed(0) #keras training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e69164d",
   "metadata": {},
   "source": [
    "# Dictionary with sample id and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce798887",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_array = np.load(labels_path)\n",
    "labels = dict()\n",
    "\n",
    "for row in labels_array:\n",
    "    labels[row[0]] = int(row[1])\n",
    "\n",
    "    \n",
    "del labels_array\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb20ae0",
   "metadata": {},
   "source": [
    "# Train, val, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66a6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open train, val, test split dictionary\n",
    "with open(split_path, \"r\") as fp:\n",
    "    train_val_test_dict = json.load(fp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84331db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create validation set and store in memory\n",
    "def set_generation(val_or_test, train_val_test_dict, labels, dim = (2500, 8)):\n",
    "    n_samples = len(train_val_test_dict[val_or_test])\n",
    "\n",
    "    #Initialise\n",
    "    X = np.empty((n_samples, dim[0], dim[1]))\n",
    "    y = np.empty((n_samples), dtype = int)\n",
    "\n",
    "    #Generate data\n",
    "    for i, ID in enumerate(train_val_test_dict[val_or_test]):\n",
    "        #store sample\n",
    "        X[i,] = np.load(samples_path + ID +\".npy\")\n",
    "\n",
    "        #store class\n",
    "        y[i] = labels[ID]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cde50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = set_generation(\"val\", train_val_test_dict, labels, (2500, 8))\n",
    "y_val = keras.utils.to_categorical(y_val, 2)\n",
    "\n",
    "#X_test, y_test = set_generation(\"test\", train_val_test_dict, labels, (2500, 8))\n",
    "#y_test = keras.utils.to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3d74f8",
   "metadata": {},
   "source": [
    "# Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0957461",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_val_test_dict[\"train\"]))\n",
    "print(len(train_val_test_dict[\"val\"]))\n",
    "print(len(train_val_test_dict[\"test\"]))\n",
    "print(len(train_val_test_dict[\"train\"]) + len(train_val_test_dict[\"val\"]) + len(train_val_test_dict[\"test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc71787",
   "metadata": {},
   "source": [
    "# Generate McFly Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648dae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_models:\n",
    "    X_train_shape = (len(train_val_test_dict[\"train\"]), 2500, 8)\n",
    "    models = mcfly.modelgen.generate_models(X_train_shape, \n",
    "                                           number_of_classes = 2,\n",
    "                                           number_of_models = n_models,\n",
    "                                           metrics = [\"accuracy\"])\n",
    "    \n",
    "    models_to_print = range(len(models))\n",
    "    for i, item in enumerate(models):\n",
    "        if i in models_to_print:\n",
    "            model, params, model_types = item\n",
    "            print(\"--------------------------------------------------------------------\")\n",
    "            print(\"Model\" + str(i))\n",
    "            print(\"  \")\n",
    "            print(\"Hyperparameters:\")\n",
    "            print(params)\n",
    "            print(\"  \")\n",
    "            print(\"Model description:\")\n",
    "            model.summary()\n",
    "            print(\"  \")\n",
    "            print(\"Model type:\")\n",
    "            print(model_types)\n",
    "            print(\" \") \n",
    "\n",
    "            for key, value in params.items():\n",
    "                if isinstance(value, np.ndarray):\n",
    "                    params[key] = value.tolist()\n",
    "\n",
    "            name = \"Model\" + str(i)\n",
    "            model_type = {\"type\": model_types}\n",
    "\n",
    "            with open(archi_path + name + \".json\", \"w\") as f:\n",
    "                json.dump(model.to_json(), f)\n",
    "\n",
    "            with open(params_path + name + \".json\", \"w\") as f:\n",
    "                json.dump(params, f)\n",
    "\n",
    "            with open(type_path + name + \".json\", \"w\") as f:\n",
    "                json.dump(model_type, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d92de56",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):    \n",
    "\n",
    "    def __init__(self, list_IDs, labels, batch_size = 32, dim = (2500,), n_channels = 8, n_classes=2, shuffle = True):\n",
    "        #\"Initialization\"\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        #number of batches per epoch\n",
    "        return int(np.floor(len(self.list_IDs)/self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Generates indexes of one batch of data\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        #find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        \n",
    "        #Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)        \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        #updates indexes after each epoch\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        #Generates data containing batch_size samples\n",
    "        \n",
    "        #Initialise\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype = int)\n",
    "        \n",
    "        #Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            #store sample\n",
    "            X[i,] = np.load(samples_path + ID +\".npy\")\n",
    "            \n",
    "            #store class\n",
    "            y[i] = self.labels[ID]\n",
    "        \n",
    "        return X, keras.utils.to_categorical(y, num_classes = self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d5d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model: \n",
    "    #define parameters\n",
    "    params = {\"dim\" : (2500,),\n",
    "             \"batch_size\": 32,\n",
    "             \"n_classes\": 2,\n",
    "             \"n_channels\":8,\n",
    "             \"shuffle\" :True}\n",
    "\n",
    "    #Generators \n",
    "    training_generator = DataGenerator(train_val_test_dict[\"train\"], labels, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8798ec45",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1910ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(archi_path + model_name, \"r\") as f:\n",
    "    model_loaded = json.load(f)\n",
    "    model = keras.models.model_from_json(model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325243ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(params_path + model_name, \"r\") as f:\n",
    "    mcfly_params = json.load(f)    \n",
    "    lr = mcfly_params[\"learning_rate\"]\n",
    "    rr = mcfly_params[\"regularization_rate\"]\n",
    "    print(lr, rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215fdcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(type_path + model_name, \"r\") as f:\n",
    "    model_type = json.load(f)    \n",
    "    print(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bf34fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set F1 as metric on val set since val test is still imbalanced\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true[:,1] * y_pred[:,1], 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true[:,1], 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true[:,1]* y_pred[:,1], 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred[:,1], 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*(precision*recall)/(precision + recall + K.epsilon())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c372553",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model: \n",
    "    \n",
    "    if add_weights:    \n",
    "        #metrics = [\"accuracy\", f1_m, tfa.metrics.MatthewsCorrelationCoefficient(num_classes=2)]\n",
    "        metric = [\"accuracy\"]\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer = \"adam\", metrics = metric)  \n",
    "\n",
    "        #calculate class imbalance\n",
    "        zeroes = 0\n",
    "        ones = 0\n",
    "        for i, ID in enumerate(train_val_test_dict[\"train\"]):\n",
    "            if labels[ID] == 0:\n",
    "                zeroes = zeroes + 1\n",
    "            if labels[ID] == 1:\n",
    "                ones = ones + 1\n",
    "\n",
    "        if ones < zeroes:\n",
    "            class_weights = {0: 1., 1: zeroes/ones}\n",
    "        elif zeroes < ones:\n",
    "            class_weights = {0: ones/zeroes, 1: 1.}\n",
    "        else:\n",
    "            class_weights = {0: 1., 1: 1.}\n",
    "    else:\n",
    "        class_weights = {0: 1., 1: 1.}\n",
    "        \n",
    "    #print time    \n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(current_time)\n",
    "    \n",
    "    #train\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 5, restore_best_weights = True)\n",
    "    history = model.fit(training_generator, \n",
    "              validation_data = (X_val, y_val), \n",
    "              epochs = 20,\n",
    "              class_weight = class_weights, \n",
    "              callbacks = callback,\n",
    "              verbose = True)\n",
    "    \n",
    "    #print time\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(current_time)\n",
    "    \n",
    "    #save the model\n",
    "    model.save(trained_path)\n",
    "    \n",
    "else:\n",
    "    #load the model\n",
    "    #model = keras.models.load_model(trained_path, custom_objects = {\"f1_m\": f1_m})\n",
    "    model = keras.models.load_model(trained_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288d497d",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d084181",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probas = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no BrS would appear as 0, hence transformed to [1,0] => the first column returns 1 if no BrS, 0 otherwise\n",
    "no_BrS = y_val[:, 0]\n",
    "\n",
    "#BrS appears as 1, hence transformed to [0,1] => the second column returns 1 if BrS, 0 otherwise\n",
    "BrS = y_val[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f1b450",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85cbc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BrS_probas = pred_probas[:,1]\n",
    "BrS_predictions = pred_probas.argmax(axis = -1)\n",
    "BrS_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085d6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_BrS_probas = pred_probas[:,0]\n",
    "no_BrS_predictions = pred_probas.argmin(axis = -1)\n",
    "no_BrS_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c832d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metrics(y_true, y_pred, y_proba):\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion matrix: \")\n",
    "    print(conf_mat)\n",
    "    tn,fp,fn,tp = conf_mat.ravel()\n",
    "    print(\"tn: \", tn,\" fp: \", fp,\" fn: \", fn,\" tp: \", tp)\n",
    "    \n",
    "    print(\"\")\n",
    "    matthews = ((tp*tn) - (fp*fn)) / math.sqrt(((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "    print(\"Matthews Correlation Coefficient: \", matthews)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    print(\"\")           \n",
    "    precision_bis = tp/(tp+fp)\n",
    "    recall_bis = tp/(tp+fn)\n",
    "    f1 = 2*precision_bis*recall_bis/(precision_bis+recall_bis)\n",
    "    print(\"precision: \", precision_bis)\n",
    "    print(\"recall/sensitivity: \", recall_bis)\n",
    "    print(\"specificity: \", tn/(tn+fp))\n",
    "    print(\"accuracy: \", (tp+tn)/(tp+tn+fp+fn))    \n",
    "    print(\"f1 score: \", f1) \n",
    "\n",
    "      \n",
    "    print(\"\")\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
    "    auc_coef = auc(fpr, tpr)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    auprc = auc(recall, precision)\n",
    "    print(\"auc: \", auc_coef)\n",
    "    print(\"auprc: \", auprc)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec4074",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(BrS, BrS_predictions, BrS_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance_metrics(no_BrS, no_BrS_predictions, no_BrS_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651b3cf6",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4cd997",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model:\n",
    "    #plot train and validation loss\n",
    "    training_loss = history.history[\"loss\"]\n",
    "    validation_loss = history.history[\"val_loss\"]\n",
    "\n",
    "    #number of epochs\n",
    "    epoch_count = range(1, len(training_loss) +1)\n",
    "\n",
    "    #visualise loss history\n",
    "    f, ax = plt.subplots(figsize=(6,6))      \n",
    "    ax.plot(epoch_count, training_loss, \"r--\", label=\"Training Loss\")\n",
    "    ax.plot(epoch_count, validation_loss, \"b-\", label=\"Validation Loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Training and Validation Loss Over the Epochs\")\n",
    "    ax.legend()\n",
    "    plt.savefig(plot_path + \"Loss.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e65fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(BrS, BrS_probas)\n",
    "auc_coef = round(auc(fpr, tpr),3)\n",
    "f, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(fpr, tpr, marker=\".\", label = model_type[\"type\"] + \" - AUC: \" + str(auc_coef))\n",
    "ax.plot([0,1], [0,1], transform = ax.transAxes, linestyle=\"--\", label=\"Random Classifier\")\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "ax.set_title(\"ROC\")\n",
    "ax.legend()\n",
    "if save_plots:\n",
    "    plt.savefig(plot_path + \"ROC.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a404ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision Recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(BrS, BrS_probas)\n",
    "auprc = round(auc(recall, precision),3)\n",
    "f, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(recall, precision, marker=\".\", label = model_type[\"type\"] + \" - AUPRC: \" + str(auprc))\n",
    "ax.set_xlabel(\"Recall (Positive label: Brugada)\")\n",
    "ax.set_ylabel(\"Precision (Positive label: Brugada)\")\n",
    "ax.set_title(\"AUPRC\")\n",
    "ax.legend()\n",
    "\n",
    "if save_plots:\n",
    "    plt.savefig(plot_path + \"PrecisionRecallCurve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cfaac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calibration\n",
    "# bin data and normalise counts\n",
    "def counts_to_percentages(probabilities):\n",
    "    bin0_01 = 0\n",
    "    bin01_02=0\n",
    "    bin02_03=0\n",
    "    bin03_04=0\n",
    "    bin04_05=0\n",
    "    bin05_06=0\n",
    "    bin06_07=0\n",
    "    bin07_08=0\n",
    "    bin08_09=0\n",
    "    bin09_1=0 \n",
    "    \n",
    "    for val in probabilities:\n",
    "    \n",
    "        if val <0.1:\n",
    "            bin0_01 = bin0_01 + 1\n",
    "    \n",
    "        elif val >= 0.1 and val <0.2:\n",
    "            bin01_02= bin01_02 +1 \n",
    "    \n",
    "        elif val >= 0.2 and val <0.3:\n",
    "            bin02_03= bin02_03 +1 \n",
    "    \n",
    "        elif val >= 0.3 and val <0.4:\n",
    "                bin03_04= bin03_04 +1\n",
    "    \n",
    "        elif val >= 0.4 and val <0.5:\n",
    "                bin04_05= bin04_05 +1 \n",
    "    \n",
    "        elif val >= 0.5 and val <0.6:\n",
    "                bin05_06= bin05_06 +1 \n",
    "    \n",
    "        elif val >= 0.6 and val <0.7:\n",
    "                    bin06_07= bin06_07 +1 \n",
    "    \n",
    "        elif val >= 0.7 and val <0.8:\n",
    "                    bin07_08= bin07_08 +1 \n",
    "    \n",
    "        elif val >= 0.8 and val <0.9:\n",
    "                    bin08_09= bin08_09 +1 \n",
    "    \n",
    "        elif val >= 0.9:\n",
    "                    bin09_1= bin09_1 +1 \n",
    "                \n",
    "    counts = [bin0_01, bin01_02, bin02_03, bin03_04, bin04_05,\n",
    "             bin05_06, bin06_07, bin07_08, bin08_09, bin09_1]    \n",
    "    \n",
    "    percentages = counts/np.sum(counts) *100\n",
    "    \n",
    "    return percentages\n",
    "    \n",
    "    \n",
    "#plot calibration plot and histogram together\n",
    "def calibration_together (BrS, BrS_probas):        \n",
    "    print(\"plot curves and save in one png file\")\n",
    "    #save three plots in one png file\n",
    "    fig_index = 1\n",
    "      \n",
    "    #save three plots in one png file\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(7, 12))   \n",
    "    \n",
    "    # plot calibration curve LSTM\n",
    "    y, x = calibration_curve(BrS, BrS_probas, n_bins=10)\n",
    "\n",
    "    ax1.plot(x, y, 'C0',marker='o', linewidth=1, label= model_type[\"type\"], color = \"darkturquoise\") \n",
    "    ax1.set(xlabel= 'Predicted probability', ylabel= 'True probability in each bin')\n",
    "    \n",
    "    line = mlines.Line2D([0, 1], [0, 1], color='black', linestyle='--', linewidth=0.9, label= \"Perfectly calibrated\")\n",
    "    transform = ax1.transAxes\n",
    "    line.set_transform(transform)\n",
    "    ax1.add_line(line)     \n",
    "    ax1.legend(loc=\"upper left\")  \n",
    "  \n",
    "    #HISTOGRAMS    \n",
    "    x = np.arange(0,1,0.1)\n",
    "    \n",
    "    #Before onset LSTM\n",
    "    y = counts_to_percentages(BrS_probas)   #if instead of % want values in [0,1], do: y = counts_to_percentages(lstm_proba)/100 \n",
    "    ax2.hist(x, range = [0,1], bins=10, weights = y, label= model_type[\"type\"],\n",
    "                 histtype=\"step\", lw=3.5, color = \"darkturquoise\")\n",
    "    \n",
    "    ax2.set_xlabel(\"Mean predicted probability\")\n",
    "    ax2.set_ylabel(\"Percentage of counts\")\n",
    "    ax2.legend(loc=\"upper center\", ncol=5)\n",
    "    ax2.set_ylim([0,101]) #if instead of % want probabilities, change to [0,1]     \n",
    "\n",
    "    #plt.tight_layout()\n",
    "    if save_plots:\n",
    "        plt.savefig(plot_path + \"Calibration.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "calibration_together(BrS, BrS_probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcfd425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discrimination\n",
    "def distribution(BrS, BrS_probas):\n",
    "    #probabilities distributions graphs\n",
    "    true_1 = pd.DataFrame(BrS_probas, columns=['Predicted probabilities'])\n",
    "    true_1['labels'] = BrS.tolist()\n",
    "    true_0 = true_1.copy(deep = True) \n",
    "    indexNames = true_1[true_1['labels'] == 0].index\n",
    "    true_1.drop(indexNames , inplace=True)\n",
    "    indexNames = true_0[ true_0['labels'] == 1 ].index\n",
    "    true_0.drop(indexNames , inplace=True)\n",
    "    true_1.drop(columns=['labels'], inplace = True)\n",
    "    true_0.drop(columns=['labels'], inplace = True)\n",
    "    \n",
    "    sns.distplot(true_1['Predicted probabilities'], hist = False, kde = True,\n",
    "                 kde_kws = {'shade': True, 'linewidth': 3,\"color\": \"g\"}, label = 'Class 1')\n",
    "    plt.ylabel('Density')\n",
    "    sns.distplot(true_0['Predicted probabilities'], hist = False, kde = True,\n",
    "                     kde_kws = {'shade': True, 'linewidth': 3, \"color\": \"r\"}, label = 'Class 0')\n",
    "    plt.legend(labels=[\"BrP\",\"No BrP\"])\n",
    "    \n",
    "    if save_plots:\n",
    "        plt.savefig(plot_path + \"Discrimination.png\")\n",
    "        \n",
    "    plt.show()\n",
    "    plt.clf()    \n",
    "    return\n",
    "\n",
    "distribution(BrS, BrS_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c056ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745c469f",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfac69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_predict(trained_model):\n",
    "    #puts sample in right format for keras prediction\n",
    "    def func(sample):\n",
    "        prediction = trained_model.predict(np.transpose(sample, axes=[0,2,1]))\n",
    "        return prediction\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba8578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_sample_lime(explainer,series_ecg, num_features_ecg, n_samples, num_slices_ecg, replacement_method, predict_function = custom_predict(trained_model=model)):\n",
    "    \n",
    "    exp = explainer.explain_instance(series_ecg, predict_function, num_features=num_features_ecg, num_samples=n_samples, \n",
    "                                 num_slices=num_slices_ecg, replacement_method = \"total_mean\")\n",
    "    \n",
    "    fig = exp.as_pyplot_figure() \n",
    "    \n",
    "    return exp, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb40ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lime(series_ecg, num_slices_ecg, X_val, y_val, BrS_predictions, idx_ecg, exp):\n",
    "\n",
    "    values_per_slice_ecg = math.ceil(series_ecg.shape[1]/ num_slices_ecg)\n",
    "    no_pattern = X_val[np.where(y_val[:,1]==0)]\n",
    "    pattern = X_val[np.where(y_val[:,1]==1)]\n",
    "    lead_to_index = {\"I\": 0, \"II\": 1, \"V1\": 2, \"V2\":3,\n",
    "                    \"V3\": 4, \"V4\": 5, \"V5\": 6, \"V6\":7}\n",
    "    leads = [\"I\", \"II\", \"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\"]\n",
    "\n",
    "    labels = [\"no Brugada Pattern\", \"Brugada Pattern\"]\n",
    "    true_label = labels[int(y_val[idx_ecg,1])]\n",
    "    predicted_label = labels[BrS_predictions[idx_ecg]]\n",
    "\n",
    "    fig, axes = plt.subplots(nrows = 4, ncols = 2, figsize = (30,30), sharex = True, sharey=True)\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        ax.plot(series_ecg[i], 'b', label='Explained instance')\n",
    "        ax.plot(no_pattern[:,:,i].mean(axis=0), color='red',label='Mean of class no Brugada Pattern')\n",
    "        ax.plot(pattern[:,:,i].mean(axis=0), color='green',label='Mean of class Brugada Pattern')\n",
    "        ax.set_title(\"Lead \"+ leads[i])\n",
    "\n",
    "        for j in range(num_features_ecg):\n",
    "            feature, weight = exp.as_list()[j]        \n",
    "            feature_name_index = lead_to_index[feature.split(\" \", 2)[2]] #split at second space in feature name to take lead name as key\n",
    "\n",
    "            if feature_name_index == i:\n",
    "                start = int(feature.split(\" \", 1)[0]) * values_per_slice_ecg #int(feature.split(\" \", 1)[0]): only keep int from feature name, eg feature name (23 - II), split at \" \" (space) and keep first part (23) and take int(23)\n",
    "                end = start + values_per_slice_ecg\n",
    "                color = 'red' if weight < 0 else 'green' \n",
    "                ax.axvspan(start , end, color=color, alpha=abs(weight*10))\n",
    "\n",
    "    ax.legend(loc='lower left')\n",
    "    title = \"LIME explanation of single sample. True label: \" + true_label + \" . Predicted label: \" + predicted_label + \".\"\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    \n",
    "    if true_label == \"no Brugada Pattern\" and predicted_label == \"no Brugada Pattern\":\n",
    "        saved_title = \"True_negative.png\"\n",
    "    elif true_label == \"no Brugada Pattern\" and predicted_label == \"Brugada Pattern\":\n",
    "        saved_title = \"False_positive.png\"\n",
    "    elif true_label == \"Brugada Pattern\" and predicted_label == \"no Brugada Pattern\":\n",
    "        saved_title = \"False_negative.png\"\n",
    "    elif true_label == \"Brugada Pattern\" and predicted_label == \"Brugada Pattern\":\n",
    "        saved_title = \"True_positive.png\"\n",
    "    \n",
    "    fig.savefig(plot_path + saved_title)\n",
    "    plt.show()\n",
    "              \n",
    "    return fig\n",
    "\n",
    "#interpretation: real label is no BrP but model predicts as Brugada (false positive). Green bands correspond to\n",
    "# evidence that it's a positive sample, red bands correspond to evidence that it's a negative sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da206a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_ecg = 50 #number of lime weights\n",
    "num_slices_ecg = 1000 #number of segments of a lead\n",
    "n_samples = 50 #number of perturbated samples at a single time point\n",
    "replacement_method = \"total_mean\" #possible replacement mathods: \"noise\" (fill in noise for perturbation), \"mean\" (fill in mean of segment), \"total_mean\" (fill in mean of lead)\n",
    "explainer = LimeTimeSeriesExplainer(class_names = [\"No BrP\", \"BrP\"], signal_names= [\"I\", \"II\", \"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978f0acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#false positive\n",
    "idx_ecg = 0 #0th sample\n",
    "series_ecg = X_val[idx_ecg].T\n",
    "exp, weights_fig = explain_sample_lime(explainer,series_ecg, num_features_ecg, n_samples, num_slices_ecg, replacement_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e34314",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lime(series_ecg, num_slices_ecg, X_val, y_val, BrS_predictions, idx_ecg, exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e4b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#false negative\n",
    "idx_ecg = 361\n",
    "series_ecg = X_val[idx_ecg].T\n",
    "exp, weights_fig = explain_sample_lime(explainer,series_ecg, num_features_ecg, n_samples, num_slices_ecg, replacement_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eb0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lime(series_ecg, num_slices_ecg, X_val, y_val, BrS_predictions, idx_ecg, exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca304f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#true positive\n",
    "idx_ecg = 360\n",
    "series_ecg = X_val[idx_ecg].T\n",
    "exp, weights_fig = explain_sample_lime(explainer,series_ecg, num_features_ecg, n_samples, num_slices_ecg, replacement_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faed9e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lime(series_ecg, num_slices_ecg, X_val, y_val, BrS_predictions, idx_ecg, exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651183d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#true negative\n",
    "idx_ecg = 10\n",
    "series_ecg = X_val[idx_ecg].T\n",
    "exp, weights_fig = explain_sample_lime(explainer,series_ecg, num_features_ecg, n_samples, num_slices_ecg, replacement_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e547173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lime(series_ecg, num_slices_ecg, X_val, y_val, BrS_predictions, idx_ecg, exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a20d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
